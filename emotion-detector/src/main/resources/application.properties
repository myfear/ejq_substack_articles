# Ollama model configuration
#quarkus.langchain4j.ollama.chat-model.model-id=llava:7b
#quarkus.langchain4j.ollama.timeout=220s

quarkus.langchain4j.openai.api-key=${OPENAI_API_KEY}
quarkus.langchain4j.openai.chat-model.model-name=gpt-4.1-mini
quarkus.langchain4j.timeout=30S
#quarkus.langchain4j.openai.chat-model.log-requests=true
quarkus.langchain4j.openai.chat-model.log-responses=true


# Vert.x thread blocking timeout - increase to allow longer blocking operations
#quarkus.vertx.warning-exception-time=300000

# WebSocket configuration - increase max frame size for JPEG images
quarkus.websockets-next.server.max-frame-size=2097152

# Enable WebSocket traffic logging for debugging
quarkus.websockets-next.server.traffic-logging.enabled=true
quarkus.log.category."io.quarkus.websockets.next.traffic".level=DEBUG

# Set logging level for application classes
quarkus.log.category."com.example".level=INFO

# Enable LangChain4j debug logging to see what's sent to Ollama
quarkus.log.category."dev.langchain4j".level=DEBUG
quarkus.log.category."io.quarkiverse.langchain4j".level=DEBUG