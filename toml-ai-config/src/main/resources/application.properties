# Pick a small-ish local model to keep the demo snappy.
quarkus.langchain4j.ollama.chat-model.model-name=qwen3:1.7b
quarkus.langchain4j.ollama.chat-model.temperature=0.2

# Local inference can take time.
quarkus.langchain4j.timeout=60s

# Optional: you can log the raw traffic while learning.
quarkus.langchain4j.ollama.log-requests=true
quarkus.langchain4j.ollama.log-responses=true